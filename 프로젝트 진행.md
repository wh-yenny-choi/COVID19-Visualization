## 프로젝트

문제 해결 프로젝트 (8/6 ~ 8/20)

주식 주가, 데이터 스트링으로 받아와서 mysql , 오라클 등으로 저장,저장한 것을 대쉬보드 형태로 보여주는것이 목표

- 대시보드 만드는 것 => 시각화
- 최종 대시보드는 이후 최종 프로젝트에서 완성
- 외부 데이터들을 수집, 정제 후  저장 
  - 데이터 프레임 형태 



■ 전공 프로젝트
- 각 과정별 전공 프로젝트 주제

 **[DE] : 데이터 엔지니어링** 

 <u>[DE] 대용량 교통 데이터 처리 및 학습용 데이터 구축</u>

 [DE] 지역별 식료품 매출 데이터 전처리

 [DE] 통신 데이터 전처리 및 학습용 고객 데이터 구축

 <u>[DE] 대용량 아파트 실거래가 데이터 수집 및 이상치 제거</u>

 [DE] 한국 방문 선택 시 고려요인 추출을 위한 외래관광객 데이터 처리

[DE] 대용량 데이터 처리를 통한 여행블로그 데이터 추출



 **[DS] : 데이터 사이언스**

 [DS] 교통데이터를 활용한 교통취약지 예측 모델 구현

 [DS] 시간별 유동인구 데이터를 활용 지역별 매출 예측

 [DS] 해외통신사 데이터를 활용한 고객 성향 분석

 [DS] 국가통계포털 데이터를 활용 아파트 전세가격 예측

 [DS] 공공데이터 활용 맛집, 모범음식점 이미지 분류

 [DS] 국민여행조사 데이터를 활용한 여행 유형 분류





## 8.4

**관심분야** 

김민형 

- 도지코인, 비트코인
- 교통
- 식품
- 고객성향 분석

박경빈

- 주식
- 교육 (수학, 코딩)
- 스포츠 (야구, 축구)
- 커피 (카페 추천 앱)
- 옷 (온라인, 오프라인 매출, 흐름)

조민호

- 주식
- 게임
- 스포츠 (야구)
- 여행

최원희

- 주식
- 식료품 매출 
- 여행  (에어비앤비)
  - https://github.com/gogoj5896/2_teamproject_air_bnb_ligression



## 8.6

계획안 정하기

주제 선정

- 유니버셜

https://www.kaggle.com/dwiknrd/reviewuniversalstudio

- 유기견

https://www.kaggle.com/jmolitoris/adoptable-dogs

- 롤

https://www.kaggle.com/gyejr95/league-of-legendslol-ranked-games-2020-ver1?select=challenger_match.csv

- 한국 혐오 언어

https://www.kaggle.com/captainnemo9292/korean-extremist-website-womad-hate-speech-data

- e러닝

https://www.kaggle.com/c/learnplatform-covid19-impact-on-digital-learning/discussion?sort=votes

https://www.kaggle.com/ruchi798/covid-19-impact-on-digital-learning-eda-w-b

- 한국 코로나 바이러스

https://www.kaggle.com/kimjihoo/coronavirusdataset

https://github.com/ThisIsIsaac/Data-Science-for-COVID-19

https://www.kaggle.com/mbnb8317/ds4c-tutorial-all-about-folium-pydeck?scriptVersionId=35512489

- 책

https://www.kaggle.com/sootersaalu/amazon-top-50-bestselling-books-2009-2019

- 삼성

https://www.kaggle.com/hyunkic/south-korea-home-to-samsung-but-few-ai-experts



## 8.7

**데이터 셋**

data.go.kr 오픈 api

청사진 : 코로나 라이브



**스파크화**

- 스파크 정의? 

- 스파크를 어떻게 활용할건지?
- api 스파크 연결?

- 스파크를 이용한 파일입출력
- 다른조 트위터를 이용한 자료를 서버에 가지고 와서 하둡 에코 시스템 
- 카푸카
-  AWS
- 하둡 hdfs, 스파크 rdbms  

스파크화를 위해서, 분석하는 워크 스페이스를 스파크로 이용



**대쉬보드**

태블로 사용하면 좋다

- 하지만 제약사항이 있음

대쉬보드 플랫폼 찾아보기



**예시**

스파크 사용한 기초 프로젝트들 찾아보기



## 8.9

**참고 자료**

데이터

https://kbig.kr/portal/

[데이터 시각화를 위한 태블로]

https://www.boostcourse.org/ds121/lecture/865837?isDesc=false

https://kim-mj.tistory.com/55

sql 접속

https://emong.tistory.com/235

코로나 실시간 맵 깃허브

https://github.com/LiveCoronaDetector/livecod



**기획안 작성**



**강사님 방향성 제시**

스파크의 df형태로 저장 - 하둡의 HDFS 형태로 저장

df(테이블 형태)저장된 것을 추출하여 시각화



**조원 피드백**

csv파일로 df 구축하는 것은 의미?

- 정형 데이터를 로컬에서 만들어서, 스파크의 가상머신에서 데이터를 받아서 활용하면 의미없다



**AWS 서버 구축**

 - 실시간 코로나 데이터 저장 ( 하둡 HDFS, 반정형 데이터 )
 - 반정형 데이터를 정형 데이터로 처리
 - 클라이언트에 정형 데이터를 response 함으로써 코로나 데이터 전송
 - 목표 : 데이터를 aws에 실시간으로 업데이트 되는 서버 구축



**태블로**

- 데이터를 활용한 시각화 



## 8.10

![image-20210810151016875](프로젝트 진행.assets/image-20210810151016875.png)



1. aws사용법
2. spark사용법
3. 태블로 사용법



스칼라 - 이클립스
서버에서 서버로 바로 데이터를 넘길 수 있다.
AWS로 api를( 웹 서버) 바로 한번에 당길 수 있다.
카루카로 연동?
api - 웹서버  ==> aws 한번에 넘길 수 있다
카푸카 로 끄집어 낸다?
하둡의 HDFS에 저장을 했으면 좋겠다..
스칼라 사용 - 이클립스 필수



**정리**

1. 데이터 모델의 이해
2. 각 조 Project
   1. 프로젝트 시나리오 구성안 
   2. 데이터 수집 결과
   3. 데이터 수집 solution test결과
   4. 데이터 저장 solution test결과
3. 각 조별 발표
4. 프로젝트 진행 중 Q&A

++ Tip : Linux명령어 찾기

​				http://explainshell.com



스파크 -> 하둡 FS로 이동 가능 

- 연동 방법?

받아오는것이 자동화가 좋은데 시간이 걸릴것

- api는 aws내부의 파이썬으로 .py파일로 받아서 돌림



## 8.11

**일정**

1. OSI 7 Layer와 Internet
2. 각 조 Project
   1. 프로젝트 시나리오 구성 안
   2. 데이터 수집 결과
   3. 데이터 수집, 저장 Process 구현안 발표
3. 각 조별 발표
4. 프로젝트 진행 중 Q&A



**HDFS 입출력 실습 예제(java 버전)**

http://hisoftlab.com/4832

- hadoop file system이 어떻게 작동하는지? 



**질문**

서버에서 hdfs 작동할때 file로 자동으로 저장되는건지?

서버 내 db설치하는 경우?

- hdfs <> mysql 
  - db연결시 어떤 흐름인지 (그림으로 표현된다면?)

- db <> hdfs

- 파일 저장시 스파크쉘에서 저장?
  - 파이썬 저장 - 스파크쉘에서 파이썬 파일 불러옴
- 스파크 플랫폼이 hdfs를 사용하는 것
  - 스파크에서 hdfs로 저장을 시켜줌



**질문 정리**

1.서버에서 hdfs 명령어 썼을 때 hdfs 저장된거로 나와있는데 이 과정이 맞는 지 궁금합니다.

2.서버 내의 데이터베이스를 설치할 경우에 대해서도 물어봐야겠습니다. 제가 궁금한 것은 hdfs그 부분이랑 우리가 알고 있는 mysql을 연결시키면 어떤 흐름도가 되는지 궁금합니다.

3.DB랑 hdfs랑 경계가 애매모해서 문제가 되는 거 같습니다.둘 사이의 차이가 거의 없는 거 같은데 그러면 두 프로그램 중 하나만 사용해도 상관없는거 아닌가요?



**질문 답변**

AWS 에서 하나의 파이썬 코드로 정리
 - api 통해서 데이터 가져오고
 - 데이터를 RDD 형태로 저장
 - RDD 저장하는 코드가 나와야 하는데, 이것이 scala or python이 될 것

RDD로 만든 후 스파크 명령어로 쓰면, 일반 파일로 저장하는 것보다 빠르다

- python이 됐든, 스칼라가 됐든 spark 명령어를 통해서 저장을 하면, 일반 파일로 저장하는 것보다 훨씬 속도가 빠르다!

- 분산 처리가 가능해진다!



**데이터 처리**

pyspark로 파이썬 파일 작성 후 돌림

코드 내에서 파일 저장 등 처리

- 참고 -  PySpark로 데이터를 읽고 쓰는 방법

https://ichi.pro/ko/spark-essentials-pysparklo-deiteoleul-ilg-go-sseuneun-bangbeob-101455223728077



**++ 서버 사양**  

※ HW 

- 고주파수 인텔 제온 E5-2686 v4(Broadwell)급 프로세서, DDR4 메모리 
- 4vCPU / 30.5GB 급 RAM / 1000GB SSD 
- Ubuntu 18.04 0S 
- mxnet, tensorflow, caffe2, chainer, cntk, pytorch 등으로 이루어진 가상환경 有 

※ SW 

- Anaconda3 / python 3.7.6 -

-  spark-3.1.2-bin-hadoop3.2(단일 머신) 설치 

  -  hadoop fs -cat dfDecide.csv
  - lab18@ip-172-31-9-66:/home/lab17$ hadoop fs -cat dfDecide.csv

- 기타 : (base) Anaconda 환경에서 TensorFlow, Keras 등 

  ​		    머신러닝, 딥러닝 관련 패키지 및 라이브러리 추가 설치 가능

 

**EC2 > Local**

키파일 폴더에서 cmd실행

터미널 창 뜨면

$C:~/~/~>pscp -i "de-a5.ppk" lab18@52.78.90.155:/home/lab17/test/*.csv C:\Users\ftsv2\Desktop

>pscp -i "de-a5.ppk" lab18@52.78.90.155:/home/lab17/dfDistrict.csv C:\Users\ftsv2\Desktop              

corona.py                                                                                                            



**발표 준비 (wrap-up)**

![](프로젝트 진행.assets/KakaoTalk_20210811_163228072.png)

Spark 파트
 - api 통해서 데이터 가져오고
 - 데이터를 RDD 형태로 저장
 - RDD 저장하는 코드가 나와야 하는데, 이것이 scala or python이 될 것
 - 하나로 합치려면 pyspark가 어울릴 것으로 보이지만, 계속 진행해봐야 알 것 같다.

태블로 파트

- Spark SQL을 이용해서 태블로와 연결하는 방법을 진행하고 있고,
  이것이 힘들 경우, csv or json 파일을 태블로로 불러와서 대쉬보드화 하는 것을 고려하고 있다.



## 8.12

**일정**

1. open software 활용과 경진대회 예제

**open source software 사이트**

https://www.oss.kr/



2. 각 조 Project
   1. 데이터 수집 sample 정제 결과 발표
   2. 저장 Process 구현 안 발표
3. 각 조별 발표
4. 프로젝트 진행 중 Q&A



Spark 파트

1. 날짜별 지역별로 df 정리되는 코드
2. 파이스파크를 통해서 파일로 내보내는 코드

태블로 파트

1. 코로나 관련 정제 데이터를 태블로에서 연결
2. 지리적 정보, 날짜 정보 변수 설정
3. 대시 보드 초기 적용



**HDFS에 저장**

df\
.write\
.mode("overwrite")\
.format("com.databricks.spark.csv")\
.save("/user/kidokim509/data.csv")

**pySpark이고 DN에 pandas가 설치 되어있다면**

df.\

toPandas().\
to_csv("data.csv")



시군구의 17개의 데이터를 컬럼으로 변경 후 날짜별로 csv파일로 만듬

pyspark

파이썬 내의 pyspark라이브러리에서 rdd형태 사용가능

- hdfs에 저장
- 파이썬 코드로 만들면 어케 rdd로 만들어서 hdfs에 저장하겠는가?
- rdd저장
- 판다스 df를 스파크의 rdd df만드는것은 가능

scala에서 rdd

13장 참고



**hadoop 작동 방식 ?**

aws내에서 

파이썬에서 판다스로 내부의 df메서드를 csv 파일로 해당 hdfs저장

df가 csv 로 저장

pyspark에서 rdd- > df

> hdfs df

df_test = spark.read.csv('dfDecide.csv', header=True)
df_test.show(5)

hdfs형태는 분산되어서 저장



**HDFS에 저장**
df1\
.write\
.mode("overwrite")\
.format("com.databricks.spark.csv")\
.save("data222.csv")



위에 날짜

지도 선택을 하면 

1. 확진자 
2. 사망자

3. 격리해제

![image-20210812161420261](프로젝트 진행.assets/image-20210812161420261.png)



![image-20210812164200641](프로젝트 진행.assets/image-20210812164200641.png)

![image-20210812164253333](프로젝트 진행.assets/image-20210812164253333.png)





치명률 - 확진자의 상관관계를 시각화

- 치명률 매개변수 생성!!

100% 기준으로 누적차트로  치명 - 확진자



돌파감염 수치로 보는거?

코로나 사망자 수 <> 백신 부작용 사망자 수

- 7월 3일 기준 509명





**백신 부작용 사망자**

1800명 중 500명 1/3.8



**코로나 사망자**

16만명 중 2천명 1/8



**wrap-up**

**Spark 파트**

- pyspark 라이브러리를 통해서 스파크 RDD를 통해서 분산 저장 가능, 완료 

- Read - write까지 거의 OK

- 데이터 정제 작업 진행 완료

**태블로 파트**

1. 코로나 관련 정제 데이터를 태블로에서 연결
( raw 데이터도 가능할 것으로 판단 )
2.대쉬보드 큰 그림 구상
 — 여기까지 완료
3. 지리적 정보, 날짜 정보 변수 설정
4. 대시 보드 초기 적용

Cf) 데이터를 받아서 지도 다닥다닥 나오는거 조금 보여주는 것도 괜찮을 듯

* 내일 진행
1. 태블로 - AWS 서버 연결( spark SQL , MySQL )
2. 태블로 계속 진행



**1조**

open api를 mongoDB에 저장을해서
spark로 데이터를 처리하면
비정형 데이터가 정형 데이터로 바뀌나봄
그 상태에서 다시 MySQL로 보내서 저장



## 8.13

8.13(목) 일정

1. 빅데이터 3대 요소
2. 빅데이터 기술
3. 각 조 Project (Documanet 보강)
   1. 데이터 수집
   2. 저장 Process 구현
4. 각 조별 발표
5. 프로젝트 진행 중 qna



**<어제 회의 내용 warp-up>**

* 내일(8/13) 진행

1. 태블로 - AWS 서버 연결( spark SQL , MySQL )
2. 태블로 계속 진행



**spark 파트**

- aws서버 접속 `ubuntu` 로 로그인 → root 계정에서 mysql설치
- aws서버 내에서 계정 발급
- 각 조원들에게 접근 권한 부여, 각 local환경에서 mysql 서버 접속
- 공통 DataBase접근 성공



**태블로 파트**

- 태블로 ↔ mySQL 연동 성공
- spark 파트에서 생성한 DB에 접속하여 테이블 접근 성공



**<해야 할 것>**

**spark 파트** : pyspark의 DataFrame을 aws서버의 mysql 데이터베이스에 저장

pyspark로 mySQL 액세스

**태블로 파트** : 생성된 테이블 데이터를 활용하여 대쉬보드(시각화)제작 진행

태블로에서 HDFS 연결

- 안된다면 ?  HDFS저장→mysql에 저장, 연결된 mysql 서버에서 데이터 받아온 후 제작



**root 계정 사용법** 

- aws 서버 ubuntu 로그인 

- $sudo 

우분투 root 에서 aws내에 mysql설치

aws내에 계정 발급

권한 - 로컬 pc에서 mysql 서버 접속 - db 접근

태블로 - mysql연동



pyspark에 df를 aws서버의 mysql 데이터베이스에 저장

jupyter-notebook --ip=0.0.0.0 --no-browser --port=8907

김민형 : ogu02 / 1234
박경빈 : ogu03 / 1234
최원희 : ogu04 / 1234







> **sql 서버만들기**
>
> https://nickjoit.tistory.com/144

ubuntu 

$sudo su

@root #apt install mysql-server

@root #mysql -u root -p 1234

mysql> use mysql

>  DB changed

mysql> select user from user;

mysql> select user, authentication_string from user;

mysql> create user 'ogu05'@'%' indentified  by '1234'

> 사용자 추가 (권한추가)
> mysql > create user 사용자ID;  // 사용자 추가
>
> mysql > create user userid@localhost identified by '비밀번호';
>
> // 사용자(user)를 추가하면서 패스워드까지 설정
>
> 기존에 사용하던 계정에 외부 접근 권한을 부여하려면, Host를 '%' 로 하여 똑같은 계정을 추가한다
>
> mysql > create user 'userid'@'%' identified by '비밀번호'; // '%' 의 의미는 외부에서의 접근을 허용



**권한 주기**

grant insert on corona19.* to 'ogu05'@'%' identified by 1234;



## 8.16

1. CUDA (빅데이터 병렬처리)
2. resoultion의 이해
3. 각 조 Project (Doucumanet 보강)
   1. 데이터 수집
   2. 저장 Process 구현
4. 13 : 10분 중간 모임 (전체 회의실 입장)
5. 각 조별 발표 (17 : 10)
6. 프로젝트 진행 중 Q&A (소회의실 수시 방문)





1. 전체 코로나 데이터 (decide, district row)
   1. 날짜 이동하면서 현황 비교✔
   2. 지역 클릭시 - 확진자, 사망자 정보 표시 **(비율)**
      1. <u>확진자 - 사망자수비율 그래프</u>✔
      2. 확진자 - 사망자 축동기화 해제 그래프
   3. 심플하고 통일성 갖춘 고객별 수익 & 할인 대시보드 참조
   4. 여러 컬럼을 활용해서 더 많은 시각화자료 만들기 
2. 지역별 확진자수 데이터 (district)
   1. 각 지역별 발생 현황 ✔
   2. 수도권 - 비수도권 비교 ✔
3. 백신서울 데이터 (vaccine Seoul)
   1. 파이차트 - 맵에 다중 마크 계층 지원 참고



++

사망자 ↔ 격리해제수 



## 8.17

1. 데이터베이스 아키텍처의 이해
2. 각 조 Project (Doucument 보강)
   1. 데이터 수집 (자동화 추가)
   2. 저장 Process 구현
   3. 시각화
3. 13 : 10분 중간 모임 (전체 회의실 입장)
4. 각 조별 발표 (17 : 10)
5. 프로젝트 진행 중 Q&A (소회의실 수시 방문)



![image-20210817092234721](프로젝트 진행.assets/image-20210817092234721.png)





**Ubuntu에 Hadoop 설치**

https://datacodingschool.tistory.com/30



```
sudo adduser --ingroup hadoop hduser
sudo vi /etc/sudoers
- 사용자 등록
>> sudo bash
>> chattr -i /etc/sudoers
>> chmod u+w /etc/sudoers

```



**aws reset 진행됨** 

우분투에 mysql설치

https://m.blog.naver.com/jesang1/221993846056

https://nickjoit.tistory.com/144

```sql
mysql -u root -p
use mysql;
select * from user;
select host, user, authentication_string from user;
create database corona19;
show database
create user 'ogu01'@'%' indentified by '1234'
grant all privileges on corona19.* to ogu01@'%' indentified by '1234';
// grant all privileges on *.* to userid@'%' identified by 'password';
flush privileges;
```



우분투에 python3설치

```
pip3 install xmltodict 
pip3 install pymysql
pip3 install sqlalchemy
pip3 install --upgrade
python3 corona.py


```



**AWS ec2 Mysql 서버구축및 외부접속하기**

https://m.blog.naver.com/software705/221337666338

```
cd /etc/mysql/my
vim mysql.cnf
vim /etc/mysql.conf.d

bind-address = 0.0.0.0
```



192.168.56.1 : 3306 



## 8.18

8/18일(수) 일정

1. 각 조 Project (Doucument 보강)
   1. 데이터 수집 (자동화 추가)
   2. 저장 Process 구현
   3. 시각화 (Tableau or pytho, django)
2. 13 : 10분 중간 모임 (전체 회의실 입장)
3. 각 조별 발표 (17 : 10)
4. 프로젝트 진행 중 Q&A (소회의실 수시 방문)



**AWS Linux에서 MySQL서버 UTF-8 한글 설정**

https://donzbox.tistory.com/604

**mysql 한글설정(aws ec2 ubuntu)**

https://ms3864.tistory.com/35

```
sudo vim mysql.cnf
/etc/mysql/mysql.conf.d
python3 corona.py
```

**해결** - DB자체의 character set 변경



```
// 지역 기준점
makepoint(0,0)
min(1) - 크기로 변경

// f.year&month
DATEPART('year',[날짜]) = INT([p.year]) 
AND 
DATEPART('month',[날짜]) = INT([p. month])
```



**프로젝트 개요**

 1. 2020년부터 두드러진 코로나
 - 2020년부터 특정 지역( 서울, 대구 ) 등을 기반으로 하여 코로나가 확산되었다.
 - 특정 기간에 따라 수도권뿐만 아니라 비수도권에서도 코로나가 두드러지고 있다.
 - 현재까지도 백신 및 치료제가 부족한 상황을 계속해서 겪고 있다.

2. 코로나 데이터를 기반으로 한 의사결정
 - 코로나 바이러스 팬데믹으로 코로나 데이터를 기반으로 한 의사결정이 매우 중요해졌다.
 - 오프라인의 활동이 대부분 온라인 활동으로 옮겨갔으며, 코로나를 통한 백신 등의 산업이 발전하였다. 
 - 즉, 코로나 데이터를 어떻게 활용하느냐가 중요



**< 지역 백신 접종 데이터 url >**
https://www.data.go.kr/data/15077756/openapi.do#layer-api-guide



확진자 수 대비 사망률 변화
확진자 수가 급격하게 늘어나는 것에 비해 사망률은 전체적으로 감소하는 추세를 보인다.

서울특별시 백신 접종 현황
데이터 상 접종률을 90%에 육박하지만 실제 전체 입구 대비 접종률은 50% 미만이다.



인구수 대비 접종 비율

수도권 - 서울, 경기, 인천
충청 - 충북, 충남, 세종, 대전
영남 - 경북, 경남, 부산, 대구, 울산
호남 - 광주, 전남, 전라
강원
제주

▲0.00%;▼0.00%;0.00%;





## 8.19

8/19일(목) 일정

1. 각 조 Project (Doucument 보강)
   1. 수집 Process 점검
   2. 저장 Process 점검
   3. 시각화 (Tableau or pytho, django)
2. 13 : 10분 중간 모임 (전체 회의실 입장)
3. 각 조별 발표 (16 : 50)
4. 프로젝트 진행 중 Q&A (소회의실 수시 방문)



우선 프로젝트를 같이 진행하며 고생한 저희 5조 조원분들에게 감사하다는 말 드리고 싶습니다.
이번 프로젝트를 진행하면서 평소 관심이 많았던 Tableau를 사용할 수 있어 즐겁게 진행했습니다. AWS서버의 mysql server와 Tableau를 연결한 상태에서 시각화를 진행하며 많은 것을 배울 수 있었습니다.
또한 DE과정에서 이론으로 배운 Spark, Hadoop과 AWS서버를 직접 활용하여, 전체적인 프로그램 사용을 경험 할 수 있는 기회였습니다. 
2주 동안 진행된 프로젝트 동안 도움을 주신 김성환 강사님 감사합니다.



![image-20210820104047099](프로젝트 진행.assets/image-20210820104047099.png)

~# mysql -u root -p

mysql>use mysql;





**데이터 확인**

<img src="프로젝트 진행.assets/image-20210820103724372.png" alt="image-20210820103724372" style="zoom:67%;" />

```
//마크 - 선반에서 편집
SUM([2차 접종 수])/SUM([총인구수 (Population1)])
SUM([p. 순수 1차 접종수]) /SUM([총인구수 (Population1)])

//[p. 순수 1차 접종수]
[누적 1차 접종자 수]-[누적 2차 접종자 수]

//p. 순수 접종대상자
[접종대상자]-[누적 1차 접종자 수]

// 접종자
sum([순수 1차 접종자 수]) + sum([누적 2차 접종자 수])
```





## 8.20

**mysql에서 ERD**

database → Reverse Engineer Database → [ERD로 출력할 DB select] → 실행

**한국어 표시**

Edit → Preferences → Modeling → Appearance → Font : Korean 선택 → OK



**TRUNCATE**

TRUNCATE 또는 TRUNCATE TABLE문은 테이블에서 모든 행을 삭제하는 데이터 정의 언어이다. 데이터베이스가 가지고 있는 무결성을 유지하는 메커니즘을 생략하여 빠른 제거를 실현하고 있는 경우가 많다.





E-R다이어그램 < 적기

1번 - 코로나 현황 대시보드 위아래 색상 좀 더 밝게

2번 - 세부 내용 글자수 키울수 있는지??/ 요일별 설명한 것 처럼 이벤트에 따라 치솟는 그래프 더 말하면 좋을듯 ---> 흥미로움 인사이트 발견한거 더 길게 말하면 시간도 채우고 좋을 것 같음

3번 - 1월 > 카페 내 취식 금지 행정명령 떨어졌을 때 <

4번 - 보라색 영역 색이 이상함 , 색상 카드 크기 키우기

5번 막대그래프 상세 박스 설명 

ppt결론에서 사망률 변화 - 막대 그래프 축 변화한거 언급!!

결론 인사이트 넘 좋음 - > 이런식으로 대시보드에서 ppt결론 인사이트 말고 낼 수 있는 인사이트 도출해낸것을 화면 보며 설명하면 더 좋을 것 같음

마무리 멘트 보충?



![image-20210820151646145](프로젝트 진행.assets/image-20210820151646145.png)


데이터 엔지니어링반 발표일정 안내드립니다.
[2시~2시50분]
*발표순서 5-4-3-2-1조

[5시 5분까지]
타팀 상호평가 링크 (쉬는시간 마다 작성해주세요!)
https://forms.gle/Tc9RedbaCfPYNqzC9

[5시6분~ 25분까지]
1)LC과제제출 
ME -> [DE 1&2]문제해결 빅데이터 활용-[DE 1&2]빅데이터 활용 포트폴리오 제출
용량압축하여 제출 (조별 동일 PPT) 미제출시 미수료 ★
2)[DE 1&2]빅데이터 활용 강의만족도(강사:김성환))
3) 구글드라이브 PPT 업로드
4)팀원 평가 설문 제출 (https://forms.gle/KHaiNrRdo9vcYH727)

[5시 40분~]
시상 진행 예정

문의사항 : 유신영 매니저 (010-2677-1172)
